{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model Validation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection  import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to dataframe\n",
    "data =pd.read_excel('C:\\\\Users\\\\Acer\\\\OneDrive\\\\Desktop\\\\Bank\\\\yes_bank.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Column details and count of rows\n",
    "data.columns,len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Description about columns ####\n",
    "\n",
    "# RowNumber       - Row number in dataset, \n",
    "# CustomerId      - CustomerId given by bank, \n",
    "# Surname         - Name of account holder,\n",
    "# CreditScore     - Credit score given by the bank based on the usage of credit card and other factors,\n",
    "# Geography       - Main branch of the account,\n",
    "# Gender          - Gender of the customer,\n",
    "# Age             - Age of the customer,\n",
    "# Tenure          - How long a person, acting as a customer to the bank,\n",
    "# Balance         - Balance in the a/c,\n",
    "# NumOfProducts   - No: of products the customer opted,\n",
    "# HasCrCard       - Whether the customer is having yes bank's credit card or not,\n",
    "# IsActiveMember  - Has made any transaction in last month,\n",
    "# EstimatedSalary - Salary of the customer,\n",
    "# Exited          - Whether still a person is a customer of yes bank or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head of Dataset (Top 5 Rows)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Unwanted columns from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting unwanted columns which are not corellated to target variable\n",
    "# Delete Row Number, CustomerId & Surname\n",
    "data=data.iloc[:,3:14]\n",
    "print(data.columns)\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking datatypes to identify contineous variables\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seprate Continuous Variable in different dataframe\n",
    "columns=['CreditScore','Age', 'Balance', 'EstimatedSalary']\n",
    "df=data.loc[:, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Plot (boxplot & histogram)\n",
    "def box_hist_fun(datacolumn):\n",
    "    \n",
    "    # Cut the window in 2 parts\n",
    "    f,(ax_box, ax_hist) = plt.subplots(2, sharex=True)\n",
    "    # Add a graph in each part\n",
    "    sns.boxplot(datacolumn, ax=ax_box)\n",
    "    sns.distplot(datacolumn.dropna(), ax=ax_hist,kde=False)\n",
    "    # Remove x axis name for the boxplot\n",
    "    ax_box.set(xlabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot graph for all variables    \n",
    "def plot_graph(df):\n",
    "    \n",
    "    for i in range(0,len(df.columns)):\n",
    "        if i<len(df[df.columns]):\n",
    "            box_hist_fun(df[df.columns[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph to identify outliers\n",
    "plot_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier treatment for 'Age'\n",
    "agemedian=df['Age'].median(skipna=True)\n",
    "df['Age'] = np.where(df['Age'] >57, agemedian,df['Age'])\n",
    "data['Age'] = np.where(data['Age'] >57, agemedian,data['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier treatment for 'CreditScore'\n",
    "df.drop(df[df['CreditScore']<383].index, inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "data.drop(data[data['CreditScore']<383].index, inplace=True)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Gender (Categorical) into Continuous data\n",
    "gender=data.Gender\n",
    "cat_gen=[]\n",
    "for x in gender:\n",
    "    if x==\"Male\":\n",
    "        cat_gen.append(1)\n",
    "    else:\n",
    "        cat_gen.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing categorical variables\n",
    "%matplotlib inline\n",
    "fig=plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(2,5,1)\n",
    "plt.title('NumOfProducts')\n",
    "plt.hist(list(data['NumOfProducts']))\n",
    "plt.subplot(2,5,2)\n",
    "plt.title('HasCrCard')\n",
    "plt.hist(list(data['HasCrCard']))\n",
    "plt.subplot(2,5,3)\n",
    "plt.title('IsActiveMember')\n",
    "plt.hist(list(data['IsActiveMember']))\n",
    "plt.subplot(2,5,4)\n",
    "plt.title('Tenure')\n",
    "plt.hist(list(data['Tenure']))\n",
    "plt.subplot(2,5,5)\n",
    "plt.title('Gender')\n",
    "plt.hist(list(cat_gen))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting to visualize outlier after treatment\n",
    "plot_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Outlier Percentage \n",
    "def outlier_percent(df):\n",
    "    \n",
    "    i=0\n",
    "    for i in range(len(df.columns)):\n",
    "            df1=df[df.columns[i]]\n",
    "            df1=df1.dropna()\n",
    "            df1=np.array(df1)\n",
    "            upper_quartile =np.percentile(df1, 75)\n",
    "            lower_quartile = np.percentile(df1, 25)\n",
    "\n",
    "            iqr = upper_quartile - lower_quartile\n",
    "\n",
    "            upper_whisker = df1[df1<=upper_quartile+1.5*iqr].max()\n",
    "            lower_whisker =  df1[df1>=lower_quartile-1.5*iqr].min()\n",
    "            \n",
    "            countofmaxoutlier=np.count_nonzero(df1[ np.where( df1 >  upper_whisker)])/len(df)\n",
    "            countofminoutlier=np.count_nonzero(df1[ np.where( df1 <  lower_whisker)])/len(df)\n",
    "            outlierpercent=round((countofmaxoutlier+countofminoutlier)*100,2)\n",
    "            \n",
    "            print(df.columns[i],'\\t',outlierpercent,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final checking of outlier percentage\n",
    "outlier_percent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Null Value Treatment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Null Values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the percent of null or nan values in data\n",
    "def null_percent(df):\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    return percent_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the percentage of null values\n",
    "null_percent(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dealing with Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.CreditScore=data.CreditScore.astype('float64')\n",
    "data.Tenure=data.Tenure.astype('float64')\n",
    "data.NumOfProducts=data.NumOfProducts.astype('float64')\n",
    "data.HasCrCard=data.HasCrCard.astype('float64')\n",
    "data.IsActiveMember=data.IsActiveMember.astype('float64')\n",
    "data.Exited=data.Exited.astype('float64')\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all Numerical variable into dataframe 'df_numeric' \n",
    "numericlist=['CreditScore', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Age', 'Balance', 'EstimatedSalary', 'Exited']\n",
    "df_numeric=data.loc[:, numericlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all Object variables into dataframe 'df_dummies'\n",
    "# if variable is Ordered Categorical use - Label encoding \n",
    "# if variable is UnOrdered use - get_dummies()\n",
    "columns=['Geography', 'Gender']\n",
    "df_dummies=data.loc[:, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useage of get_dummies()\n",
    "data_cat_dummies=pd.get_dummies(df_dummies,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Categorical and Numerical dataframe \n",
    "prepro_data=pd.concat([data_cat_dummies,df_numeric],axis=1)\n",
    "print(prepro_data.dtypes)\n",
    "print(prepro_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = prepro_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Data\n",
    "prepro_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prepro_data.loc[:,'Exited'].values\n",
    "X = prepro_data.loc[:,prepro_data.columns!='Exited'].values\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = 0.2,random_state=0)\n",
    "print(\"Training set: \", X_train.shape, y_train.shape)\n",
    "print(\"Test set: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def get_learning_curve(model,name):\n",
    "  train_size, train_score, test_score = learning_curve(estimator=model, X=X, y=y, cv=10 )\n",
    "  train_score_m = np.mean(train_score, axis=1)\n",
    "  test_score_m = np.mean(test_score, axis=1)\n",
    "  plt.plot(train_size, train_score_m, 'o-', color=\"b\")\n",
    "  plt.plot(train_size, test_score_m, 'o-', color=\"r\")\n",
    "  plt.legend(('Training score', 'Test score'), loc='best')\n",
    "  plt.xlabel(\"Training Samples\")\n",
    "  plt.ylabel(\"Score\")\n",
    "  title_text = \"Learning curve for \"+name\n",
    "  plt.title(title_text)\n",
    "  plt.grid()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results compilation Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Model','Accuracy','Score','Precision','F1 Score']\n",
    "compare = pd.DataFrame(columns = col_names)\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(arr):\n",
    "  TP = arr[0][0]\n",
    "  FP = arr[1][1]\n",
    "  TN = arr[1][0]\n",
    "  FN = arr[0][1]\n",
    "  acc = (TP+FP)/(TP+FP+TN+FN)\n",
    "  pre = TP/(TP+FP)\n",
    "  rec = TP/(TP+FN)\n",
    "  f1 = 2*((pre*rec)/(pre+rec))\n",
    "  return acc, pre, rec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(cmatrix,scores):\n",
    "  print(cmatrix)\n",
    "  print(\"Mean Accuracy is                     :\",np.mean(scores))\n",
    "  print(\"Standard Deviation of accuracies is  :\",np.std(scores))\n",
    "  cmatrix = cmatrix.to_numpy()\n",
    "  return cmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without KFold Cross validation \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Modeel Accuracy\",model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without KFold Cross validation with hyper-parameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid={\"C\":np.logspace(0.1,1.0,5), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "model=LogisticRegression()\n",
    "model_cv=GridSearchCV(model,grid,cv=10)\n",
    "model_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",model_cv.best_params_)\n",
    "print(\"accuracy :\",model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Optimization\n",
    "model = LogisticRegression(C=1.0, penalty= \"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))\n",
    "get_learning_curve(model,\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K Fold CV\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "model = LogisticRegression(C=1.0, penalty= \"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "array = [[0,0],[0,0]]\n",
    "scores = []\n",
    "cv = KFold(n_splits = 10, random_state=42, shuffle = False)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    #print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #print(\"Test Index: \", test_index)\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "    c = confusion_matrix(y_test, model.predict(X_test))\n",
    "    array = array + c\n",
    "cm = pd.DataFrame(array, index = ['1', '0'], columns = ['1', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results and Put the results into the dataframe\n",
    "cm = get_results(cm,scores)\n",
    "acc, pre, rec, f1 = get_scores(cm)\n",
    "compare.loc[len(compare)] = [\"Logistic\", round(acc,2), round(pre,2), round(rec,2), round(f1,2)]\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without KFold Cross validation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without KFold Cross validation with hyper-parameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "depths = np.arange(1, 21)\n",
    "num_leafs = [5, 10, 20, 50, 100]\n",
    "parameters={'max_depth': depths, 'max_leaf_nodes': num_leafs}\n",
    "model = DecisionTreeClassifier()\n",
    "model_cv=GridSearchCV(model, param_grid=parameters,cv=10)\n",
    "model_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",model_cv.best_params_)\n",
    "print(\"accuracy :\",model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How gridsearch works\n",
    "1. Try every combination of your parameter grid\n",
    "2. For each of them it will do a K-fold cross validation - By default CV = 3\n",
    "3. Select the best available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Optimization\n",
    "model = DecisionTreeClassifier(max_depth=8, max_leaf_nodes=50)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))\n",
    "get_learning_curve(model,\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K Fold CV\n",
    "## Go through uniform and stratified k fold.\n",
    "from sklearn.model_selection import KFold\n",
    "model = DecisionTreeClassifier(max_depth=8, max_leaf_nodes=50)\n",
    "model.fit(X_train, y_train)\n",
    "array = [[0,0],[0,0]]\n",
    "scores = []\n",
    "cv = KFold(n_splits = 10, random_state=42, shuffle = False)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    #print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #print(\"Test Index: \", test_index)\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "    c = confusion_matrix(y_test, model.predict(X_test))\n",
    "    array = array + c\n",
    "cm = pd.DataFrame(array, index = ['1', '0'], columns = ['1', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results and Put the results into the dataframe\n",
    "cm = get_results(cm,scores)\n",
    "acc, pre, rec, f1 = get_scores(cm)\n",
    "compare.loc[len(compare)] = [\"Decision Tree\", round(acc,2), round(pre,2), round(rec,2), round(f1,2)]\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Hyper-parameter optimization\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))\n",
    "get_learning_curve(model,\"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There isn't a hyper-parameter to tune, so you have nothing to grid search over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K Fold CV\n",
    "## Go through uniform and stratified k fold.\n",
    "from sklearn.model_selection import KFold\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "array = [[0,0],[0,0]]\n",
    "scores = []\n",
    "cv = KFold(n_splits = 10, random_state=42, shuffle = False)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    #print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #print(\"Test Index: \", test_index)\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "    c = confusion_matrix(y_test, model.predict(X_test))\n",
    "    array = array + c\n",
    "cm = pd.DataFrame(array, index = ['1', '0'], columns = ['1', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results and Put the results into the dataframe\n",
    "cm = get_results(cm,scores)\n",
    "acc, pre, rec, f1 = get_scores(cm)\n",
    "compare.loc[len(compare)] = [\"Naive Bayes\", round(acc,2), round(pre,2), round(rec,2), round(f1,2)]\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Nearest Neighbour classifier (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without KFold Cross validation \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "\n",
    "# K\n",
    "for i in range(1,50):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.plot(range(1,50), error_rate, color='blue', linestyle='dashed',marker='o', markerfacecolor='red',markersize=10)\n",
    "plt.title('Error_rate vs k-value')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Error rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without KFold Cross validation with hyper-parameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "k_range = list(range(1,31))\n",
    "parameters = {'n_neighbors':k_range,\n",
    "              'leaf_size':[1,3,5],\n",
    "              'algorithm':['auto', 'kd_tree'],\n",
    "              'weights': ['uniform', 'distance']}\n",
    "\n",
    "model= KNeighborsClassifier()\n",
    "model_cv=GridSearchCV(model, param_grid=parameters,cv=10)\n",
    "model_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",model_cv.best_params_)\n",
    "print(\"accuracy :\",model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Optimization\n",
    "model = KNeighborsClassifier(algorithm=\"auto\", leaf_size=1, weights='uniform', n_neighbors= 30)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))\n",
    "get_learning_curve(model,\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K Fold CV\n",
    "## Go through uniform and stratified k fold.\n",
    "from sklearn.model_selection import KFold\n",
    "model = KNeighborsClassifier(algorithm=\"auto\", leaf_size=1, weights='uniform', n_neighbors= 30)\n",
    "model.fit(X_train, y_train)\n",
    "array = [[0,0],[0,0]]\n",
    "scores = []\n",
    "cv = KFold(n_splits = 10, random_state=42, shuffle = False)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    #print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #print(\"Test Index: \", test_index)\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "    c = confusion_matrix(y_test, model.predict(X_test))\n",
    "    array = array + c\n",
    "cm = pd.DataFrame(array, index = ['1', '0'], columns = ['1', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results and Put the results into the dataframe\n",
    "cm = get_results(cm,scores)\n",
    "acc, pre, rec, f1 = get_scores(cm)\n",
    "compare.loc[len(compare)] = [\"KNN\", round(acc,2), round(pre,2), round(rec,2), round(f1,2)]\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without KFold Cross validation \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without KFold Cross validation with hyper-parameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\n",
    "        'min_child_weight': [1, 2, 3, 5, 10],\n",
    "        'gamma': [0.5, 1, 2.2, 2.3, 5],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'reg_lambda': [0.1,0.3,0.5]\n",
    "        }\n",
    "\n",
    "model= XGBClassifier()\n",
    "model_cv=GridSearchCV(model, param_grid=parameters,cv=10)\n",
    "model_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",model_cv.best_params_)\n",
    "print(\"accuracy :\",model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Optimization\n",
    "model = XGBClassifier(algorithm= 'auto', min_child_weight= 1, gamma=2.3 , max_depth=5 , reg_lambda=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))\n",
    "get_learning_curve(model,\"XG Boost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K Fold CV\n",
    "## Go through uniform and stratified k fold.\n",
    "from sklearn.model_selection import KFold\n",
    "model = XGBClassifier(algorithm= 'auto', min_child_weight= 1, gamma=2.3 , max_depth=5 , reg_lambda=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "array = [[0,0],[0,0]]\n",
    "scores = []\n",
    "cv = KFold(n_splits = 10, random_state=42, shuffle = False)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    #print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #print(\"Test Index: \", test_index)\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "    c = confusion_matrix(y_test, model.predict(X_test))\n",
    "    array = array + c\n",
    "cm = pd.DataFrame(array, index = ['1', '0'], columns = ['1', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results and Put the results into the dataframe\n",
    "cm = get_results(cm,scores)\n",
    "acc, pre, rec, f1 = get_scores(cm)\n",
    "compare.loc[len(compare)] = [\"XG Boost\", round(acc,2), round(pre,2), round(rec,2), round(f1,2)]\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without KFold Cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without KFold Cross validation with hyper-parameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = { \n",
    "    'n_estimators': [10, 20, 40, 60, 80, 100],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'bootstrap': [True],\n",
    "    'criterion' :['gini', 'entropy'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12]\n",
    "}\n",
    "model = RandomForestClassifier()\n",
    "model_cv=GridSearchCV(model, param_grid=parameters,cv=10)\n",
    "model_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",model_cv.best_params_)\n",
    "print(\"accuracy :\",model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Optimization\n",
    "model = RandomForestClassifier(criterion=\"entropy\", max_depth = 8, max_features=\"sqrt\", n_estimators=40, bootstrap= True, min_samples_leaf= 4, min_samples_split= 8)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))\n",
    "get_learning_curve(model,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K Fold CV\n",
    "## Go through uniform and stratified k fold.\n",
    "from sklearn.model_selection import KFold\n",
    "model = model = RandomForestClassifier(criterion=\"entropy\", max_depth = 8, max_features=\"sqrt\", n_estimators=40, bootstrap= True, min_samples_leaf= 4, min_samples_split= 8)\n",
    "model.fit(X_train, y_train)\n",
    "array = [[0,0],[0,0]]\n",
    "scores = []\n",
    "cv = KFold(n_splits = 10, random_state=42, shuffle = False)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    #print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #print(\"Test Index: \", test_index)\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "    c = confusion_matrix(y_test, model.predict(X_test))\n",
    "    array = array + c\n",
    "cm = pd.DataFrame(array, index = ['1', '0'], columns = ['1', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results and Put the results into the dataframe\n",
    "cm = get_results(cm,scores)\n",
    "acc, pre, rec, f1 = get_scores(cm)\n",
    "compare.loc[len(compare)] = [\"Random Forest\", round(acc,2), round(pre,2), round(rec,2), round(f1,2)]\n",
    "compare.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Hyper-parameter optimization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without KFold Cross validation with hyper-parameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters= {'kernel':['linear','rbf'], 'C':[1,0.25,0.5,0.75],'gamma':[1,2,3,'auto'], \n",
    "            'decision_function_shape':('ovo','ovr'),\n",
    "            'shrinking':(True,False)}\n",
    "model = SVC()\n",
    "model_cv=GridSearchCV(model, param_grid= parameters,cv=10)\n",
    "model_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",model_cv.best_params_)\n",
    "print(\"accuracy :\",model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above code reuired lots of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Hyper-parameter optimization\n",
    "model = SVC(C=1.0 , gamma= 'auto', kernel='rbf', decision_function_shape='ovr', shrinking=True)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "print(\" Model Accuracy\",model.score(X_test, y_test))\n",
    "get_learning_curve(model,\"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K Fold CV\n",
    "## Go through uniform and stratified k fold.\n",
    "from sklearn.model_selection import KFold\n",
    "model = SVC(C=1.0 , gamma= 'auto', kernel='rbf', decision_function_shape='ovr', shrinking=True)\n",
    "model.fit(X_train, y_train)\n",
    "array = [[0,0],[0,0]]\n",
    "scores = []\n",
    "cv = KFold(n_splits = 10, random_state=42, shuffle = False)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    #print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #print(\"Test Index: \", test_index)\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "    c = confusion_matrix(y_test, model.predict(X_test))\n",
    "    array = array + c\n",
    "cm = pd.DataFrame(array, index = ['1', '0'], columns = ['1', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results and Put the results into the dataframe\n",
    "cm = get_results(cm,scores)\n",
    "acc, pre, rec, f1 = get_scores(cm)\n",
    "compare.loc[len(compare)] = [\"SVM\", round(acc,2), round(pre,2), round(rec,2), round(f1,2)]\n",
    "compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "# 1) app.py - for generating REST APIs - go through\n",
    "# 2) FLASK / DJango \n",
    "# 3) Index HTML - Form \n",
    "# 4) request file\n",
    "# 5) model.py --> model.pkl\n",
    "# 6) requierment -> update with the versions of python packages \n",
    "\n",
    "#Step 2: Code compilation - python app.py ==> URL Local host\n",
    "\n",
    "#Step 3: Upload the files on github\n",
    "\n",
    "#Step 4: Connect github to Heroku and Deploy to get global URL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
